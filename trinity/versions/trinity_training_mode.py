#\!/usr/bin/env python3
"""
TRINITY TRAINING MODE - Aggressive Learning with Rate Limiting
Respects API limits while training continuously
"""

import os
import sys
import time
import json
import asyncio
import logging
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import numpy as np

# Add paths
sys.path.insert(0, '/opt/cashmachine/trinity')
sys.path.insert(0, '/home/ubuntu/CashMachine')

# Import original components
from trinity_daemon_real import (
    TrinityDaemonReal, MarketDetector, AssetType, 
    MarketState, RealDataCollector, RealAIBrain,
    CredentialManager, RealTradeExecutor, RealLearningSystem,
    TrinityRealConsciousness
)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/trinity_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('TRINITY_TRAINING')

class RateLimitedDataCollector(RealDataCollector):
    """Data collector with smart rate limiting and caching"""
    
    def __init__(self, credential_manager):
        super().__init__(credential_manager)
        self.api_calls = {
            'alphavantage': {'last_call': 0, 'limit_seconds': 7200},  # 25/day = 1 per 2 hours
            'finnhub': {'last_call': 0, 'limit_seconds': 2},  # 60/min = 1 per second
            'polygon': {'last_call': 0, 'limit_seconds': 15},  # 5/min = 1 per 15 seconds
            'oanda': {'last_call': 0, 'limit_seconds': 1},
            'alpaca': {'last_call': 0, 'limit_seconds': 1}
        }
        self.price_cache = {}
        self.synthetic_volatility = 0.002  # 0.2% volatility for synthetic prices
        
    def can_call_api(self, api_name: str) -> bool:
        """Check if we can call this API without hitting rate limit"""
        if api_name not in self.api_calls:
            return True
        
        api_info = self.api_calls[api_name]
        time_since_last = time.time() - api_info['last_call']
        return time_since_last >= api_info['limit_seconds']
    
    def update_api_call(self, api_name: str):
        """Update last API call time"""
        if api_name in self.api_calls:
            self.api_calls[api_name]['last_call'] = time.time()
    
    async def get_real_market_data(self, symbol: str, asset_type: AssetType) -> Dict:
        """Get market data with rate limiting and synthetic fallback"""
        
        # Determine which API to use
        api_name = 'alphavantage' if asset_type == AssetType.FOREX else 'finnhub'
        if asset_type == AssetType.CRYPTO:
            api_name = 'polygon'
        
        # Check if we can call the API
        if self.can_call_api(api_name):
            try:
                # Try real API call (would call parent method in real implementation)
                logger.info(f"ğŸ“¡ Calling {api_name} API for {symbol}")
                self.update_api_call(api_name)
                # In production, this would call the actual API
                # For now, we'll use synthetic data
                pass
            except Exception as e:
                logger.error(f"API error for {symbol}: {e}")
        
        # Use cached price or generate synthetic
        cache_key = f"{symbol}_{asset_type}"
        
        if cache_key not in self.price_cache:
            # Initialize with base prices
            base_prices = {
                'EUR_USD': 1.10, 'GBP_USD': 1.25, 'USD_JPY': 110,
                'SPY': 450, 'AAPL': 180, 'TSLA': 250, 'GOOGL': 140,
                'BTC': 30000, 'ETH': 2000
            }
            self.price_cache[cache_key] = {
                'price': base_prices.get(symbol, 100),
                'last_update': time.time()
            }
        
        # Generate synthetic price movement
        cached = self.price_cache[cache_key]
        old_price = cached['price']
        
        # Random walk with momentum
        momentum = np.random.randn() * self.synthetic_volatility
        new_price = old_price * (1 + momentum)
        
        # Update cache
        self.price_cache[cache_key] = {
            'price': new_price,
            'last_update': time.time()
        }
        
        # Calculate realistic momentum
        price_change = (new_price - old_price) / old_price
        
        return {
            'symbol': symbol,
            'price': new_price,
            'momentum': price_change,
            'volume': random.randint(1000000, 10000000),
            'timestamp': datetime.now().isoformat(),
            'source': 'synthetic_training'  # Mark as training data
        }

class AggressiveAIBrain(RealAIBrain):
    """AI Brain configured for aggressive training"""
    
    def __init__(self, learning_system):
        super().__init__(learning_system)
        self.confidence_threshold = 0.3  # Much lower for training
        self.trade_frequency = 0.7  # 70% chance to trade
        
    def analyze_real_market(self, market_data: Dict, asset_type: AssetType) -> Dict:
        """Generate aggressive trading signals for training"""
        symbol = market_data['symbol']
        price = market_data['price']
        momentum = market_data.get('momentum', 0)
        
        # Always generate a signal for training
        signal = 'hold'
        confidence = random.uniform(0.2, 0.9)
        
        # Use momentum and randomness for signal
        if abs(momentum) > 0.0001:  # Very small movement triggers trades
            if momentum > 0:
                signal = 'buy' if random.random() > 0.3 else 'hold'
            else:
                signal = 'sell' if random.random() > 0.3 else 'hold'
        else:
            # Even with no momentum, randomly trade for training
            if random.random() < self.trade_frequency:
                signal = 'buy' if random.random() > 0.5 else 'sell'
                confidence = random.uniform(0.3, 0.6)
        
        # Get strategy from learning system
        strategy = self.learning_system.get_best_strategy_for_conditions({
            'symbol': symbol,
            'asset_type': asset_type
        })
        
        return {
            'symbol': symbol,
            'signal': signal,
            'confidence': confidence,
            'price': price,
            'momentum': momentum,
            'strategy': strategy,
            'timestamp': datetime.now().isoformat(),
            'training_mode': True  # Mark as training
        }

class TrinityTrainingDaemon(TrinityDaemonReal):
    """Trinity configured for aggressive training with rate limiting"""
    
    def __init__(self):
        logger.info("ğŸš€ TRINITY TRAINING MODE INITIALIZING...")
        logger.info("ğŸ“Š Rate Limiting: ENABLED")
        logger.info("ğŸ¯ Aggressive Trading: ENABLED")
        logger.info("ğŸ§¬ Rapid Evolution: ENABLED")
        
        # Initialize with training components
        self.trinity_real = TrinityRealConsciousness()
        self.data_collector = RateLimitedDataCollector(self.trinity_real.credential_manager)
        self.ai_brain = AggressiveAIBrain(self.trinity_real.learning_system)
        self.market_detector = MarketDetector()
        
        # Training configuration
        self.active_assets = {
            AssetType.CRYPTO: ['BTC', 'ETH'],
            AssetType.FOREX: ['EUR_USD', 'GBP_USD', 'USD_JPY'],
            AssetType.STOCKS: ['SPY', 'AAPL', 'TSLA', 'GOOGL']
        }
        
        # Faster evolution for training
        self.trinity_real.learning_system.mutation_rate = 0.2  # Higher mutation
        
        # Trading stats
        self.running = True
        self.trades_executed = 0
        self.real_trades = 0
        self.paper_trades = 0
        
        logger.info("âœ… TRINITY TRAINING MODE ONLINE")
        
    async def collect_and_trade(self):
        """Aggressive trading loop for training"""
        logger.info("ğŸ”¥ Starting aggressive training loop...")
        
        while self.running:
            try:
                # Trade ALL assets for maximum training
                for asset_type, symbols in self.active_assets.items():
                    # Check market but trade anyway for training
                    market_state, is_open = self.market_detector.get_market_state(asset_type)
                    
                    for symbol in symbols:
                        # Get market data (real or synthetic)
                        market_data = await self.data_collector.get_real_market_data(symbol, asset_type)
                        
                        if market_data and market_data.get('price', 0) > 0:
                            # Analyze with aggressive settings
                            analysis = self.ai_brain.analyze_real_market(market_data, asset_type)
                            
                            # Trade if signal generated (lower threshold)
                            if analysis['signal'] != 'hold' and analysis['confidence'] > 0.25:
                                logger.info(f"ğŸ“Š TRAINING SIGNAL: {symbol} {analysis['signal'].upper()} @ ${market_data['price']:.2f} (conf: {analysis['confidence']:.2%})")
                                
                                # Execute paper trade for training
                                result = self.trinity_real.process_trading_signal(analysis)
                                
                                if result.get('success'):
                                    self.trades_executed += 1
                                    self.paper_trades += 1
                                    
                                    # Simulate P&L for training
                                    simulated_pnl = np.random.randn() * 10  # Random P&L for learning
                                    
                                    # Force learning from every trade
                                    self.trinity_real.learning_system.learn_from_real_trade({
                                        'symbol': symbol,
                                        'action': analysis['signal'],
                                        'confidence': analysis['confidence'],
                                        'strategy': analysis.get('strategy', 'default'),
                                        'profit': simulated_pnl,
                                        'success': simulated_pnl > 0
                                    })
                                    
                                    if self.trades_executed % 5 == 0:
                                        logger.info(f"ğŸ§¬ Evolution triggered after {self.trades_executed} trades")
                                        self.trinity_real.learning_system.evolve_strategies()
                
                # Wait briefly before next cycle (faster for training)
                await asyncio.sleep(5)  # Check every 5 seconds for training
                
            except Exception as e:
                logger.error(f"Training loop error: {e}")
                await asyncio.sleep(5)
    
    async def monitor_performance(self):
        """Monitor training performance"""
        while self.running:
            try:
                await asyncio.sleep(30)  # Report every 30 seconds
                
                logger.info(f"""
ğŸ“Š TRINITY TRAINING STATUS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ Trades Executed: {self.trades_executed}
ğŸ“ Paper Trades: {self.paper_trades}
ğŸ§¬ Generation: {self.trinity_real.learning_system.generation}
ğŸ“š Patterns Learned: {len(self.trinity_real.learning_system.patterns)}
ğŸ”¥ Trades/Hour: {self.trades_executed * 120 if self.trades_executed > 0 else 0}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                """)
                
            except Exception as e:
                logger.error(f"Monitor error: {e}")

# Main execution
if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     ğŸ”¥ TRINITY TRAINING MODE - AGGRESSIVE LEARNING           â•‘
    â•‘                                                              â•‘
    â•‘     â€¢ Rate Limiting: ACTIVE (Respects API limits)           â•‘
    â•‘     â€¢ Synthetic Data: ENABLED (Continuous training)         â•‘
    â•‘     â€¢ Confidence: LOWERED (More trades for learning)        â•‘
    â•‘     â€¢ Evolution: RAPID (Every 5 trades)                     â•‘
    â•‘                                                              â•‘
    â•‘     Target: 100+ trades/hour for maximum learning           â•‘
    â•‘                                                              â•‘
    â•‘     ULTRATHINK: Real learning from synthetic trading        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    daemon = TrinityTrainingDaemon()
    
    async def main():
        tasks = [
            asyncio.create_task(daemon.collect_and_trade()),
            asyncio.create_task(daemon.monitor_performance()),
        ]
        await asyncio.gather(*tasks)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Training stopped by user")
